---
title: "p8105_hw5_ga2612"
author: "Lupe Antonio"
date: "11/12/2023"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(purrr)
library(broom)
```

## Problem 1

```{r, message = FALSE}
#loading data
homicide_data <- read_csv('data_hw5/homicide_data.csv')
```







## Problem 2

```{r, message=FALSE}
#loading data
file_names <- list.files(path = 'data_hw5/p2_data') %>%
  as_tibble() %>%
  rename(file_name = value) %>%
  #iterating over file names, reading file, and create new variable in df
  mutate(
    data = map(str_c('./data_hw5/p2_data/', file_name), read_csv)) %>%
  unnest(cols = data) %>%
  pivot_longer(week_1:week_8,
               names_to = 'week',
               values_to = 'value') %>%
  mutate(
    #creating arm variable for either control or experimental
    arm = case_when(
      str_detect(file_name, 'con') ~ 'control',
      TRUE ~ 'experimental'),
    
    #creating subject id variable
    id = as.numeric(str_sub(file_name, start = 5, end = 6)),
    
    #updating week to numeric
    week = as.numeric(str_sub(week, start = 6, end = 6)))
  
```


```{r}
#creating plot
file_names %>%
  ggplot(aes(x = week, y = value, color = arm, group = interaction(id, arm))) +
  geom_line() +
  ggtitle('Observations per subjects between week 1 and 8')
```

Based on the plot above, we can see that over time the participants with the experimental arm had increasing values over time. While those with the control arm had relatively consistent values over time, not much fluctuation. 


## Problem 3

When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

First set the following design elements:

Fix n=30
Fix σ=5
Set μ=0
. Generate 5000 datasets from the model

x∼Normal[μ,σ]

For each dataset, save μ̂ 
 and the p-value arising from a test of H:μ=0
 using α=0.05
. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}
, and complete the following:

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ
 on the x axis. Describe the association between effect size and power.
Make a plot showing the average estimate of μ̂ 
 on the y axis and the true value of μ
 on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂ 
 only in samples for which the null was rejected on the y axis and the true value of μ
 on the x axis. Is the sample average of μ̂ 
 across tests for which the null is rejected approximately equal to the true value of μ
? Why or why not?



Need: to save mu and p-value from t.test, null mu=0 and alpha=0.05

```{r}
set.seed(100)

#setting design elements
n <- 30
sigma <- 5
mu <- 0
simulations <- 5000

alternate_mus <- c(1, 2, 3, 4, 5, 6)

sim_data


expand_grid(
  sample_size = 30,
  iter = 1:5000,
  sigma = 5,
  alternate_mus = c(1, 2, 3, 4, 5, 6)) %>%
  mutate(
    values_df = map(alternate_mus, t.test(alternate_mus, mu = 0, conf.level = 0.95)))

```




```{r}
sim_mean_pval <- function(mean, n = 30,sigma = 5){
  sim_data = tibble(
    x = rnorm(n = n, mean = 0, sd = sigma))
  
  mean_pval = sim_data %>%
    pull(x) %>%
    #.95 for conf.level, 0 mu is already preset. 
    t.test() %>%
    tidy() %>%
    select(estimate, p.value)
  
  return(mean_pval)}

sim_mean_pval(mean = 1)
```

